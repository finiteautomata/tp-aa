{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with text \n",
    "\n",
    "[Tutorial](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#extracting-features-from-text-files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "\n",
    "categories = ['alt.atheism', 'soc.religion.christian']\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train',categories=categories, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'soc.religion.christian']\n",
      "From: nigel.allen@canrem.com (Nigel Allen)\n",
      "Subject: library of congress to host dead sea scroll symposium april 21-22\n",
      "Lines: 96\n",
      "\n",
      "\n",
      " Library of Congress to Host Dead Sea Scroll Symposium April 21-22\n",
      " To: National and Assignment desks, Daybook Editor\n",
      " Contact: John Sullivan, 202-707-9216, or Lucy Suddreth, 202-707-9191\n",
      "          both of the Library of Congress\n",
      "\n",
      "   WASHINGTON, April 19  -- A symposium on the Dead Sea \n",
      "Scrolls will be held at the Library of Congress on Wednesday,\n",
      "April 21, and Thursday, April 22.  The two-day program, cosponsored\n",
      "by the library and Baltimore Hebrew University, with additional\n",
      "support from the Project Judaica Foundation, will be held in the\n",
      "library's Mumford Room, sixth floor, Madison Building.\n",
      "   Seating is limited, and admission to any session of the symposium\n",
      "must be requested in writing (see Note A).\n",
      "   The symposium will be held one week before the public opening of a\n",
      "major exhibition, \"Scrolls from the Dead Sea: The Ancient Library of\n",
      "Qumran and Modern Scholarship,\" that opens at the Library of Congress\n",
      "on April 29.  On view will be fragmentary scrolls and archaeological\n",
      "artifacts excavated at Qumran, on loan from the Israel Antiquities\n",
      "Authority.  Approximately 50 items from Library of Congress special\n",
      "collections will augment these materials.  The exhibition, on view in\n",
      "the Madison Gallery, through Aug. 1, is made possible by a generous\n",
      "gift from the Project Judaica Foundation of Washington, D.C.\n",
      "   The Dead Sea Scrolls have been the focus of public and scholarly\n",
      "interest since 1947, when they were discovered in the desert 13 miles\n",
      "east of Jerusalem.  The symposium will explore the origin and meaning\n",
      "of the scrolls and current scholarship.  Scholars from diverse\n",
      "academic backgrounds and religious affiliations, will offer their\n",
      "disparate views, ensuring a lively discussion.\n",
      "   The symposium schedule includes opening remarks on April 21, at\n",
      "2 p.m., by Librarian of Congress James H. Billington, and by\n",
      "Dr. Norma Furst, president, Baltimore Hebrew University.  Co-chairing\n",
      "the symposium are Joseph Baumgarten, professor of Rabbinic Literature\n",
      "and Institutions, Baltimore Hebrew University and Michael Grunberger,\n",
      "head, Hebraic Section, Library of Congress.\n",
      "   Geza Vermes, professor emeritus of Jewish studies, Oxford\n",
      "University, will give the keynote address on the current state of\n",
      "scroll research, focusing on where we stand today. On the second\n",
      "day, the closing address will be given by Shmaryahu Talmon, who will\n",
      "propose a research agenda, picking up the theme of how the Qumran\n",
      "studies might proceed.\n",
      "   On Wednesday, April 21, other speakers will include:\n",
      "\n",
      "   -- Eugene Ulrich, professor of Hebrew Scriptures, University of\n",
      "Notre Dame and chief editor, Biblical Scrolls from Qumran, on \"The\n",
      "Bible at Qumran;\"\n",
      "   -- Michael Stone, National Endowment for the Humanities\n",
      "distinguished visiting professor of religious studies, University of\n",
      "Richmond, on \"The Dead Sea Scrolls and the Pseudepigrapha.\"\n",
      "   -- From 5 p.m. to 6:30 p.m. a special preview of the exhibition\n",
      "will be given to symposium participants and guests.\n",
      "\n",
      "   On Thursday, April 22, beginning at 9 a.m., speakers will include:\n",
      "\n",
      "   -- Magen Broshi, curator, shrine of the Book, Israel Museum,\n",
      "Jerusalem, on \"Qumran: The Archaeological Evidence;\"\n",
      "   -- P. Kyle McCarter, Albright professor of Biblical and ancient\n",
      "near Eastern studies, The Johns Hopkins University, on \"The Copper\n",
      "Scroll;\"\n",
      "   -- Lawrence H. Schiffman, professor of Hebrew and Judaic studies,\n",
      "New York University, on \"The Dead Sea Scrolls and the History of\n",
      "Judaism;\" and\n",
      "   -- James VanderKam, professor of theology, University of Notre\n",
      "Dame, on \"Messianism in the Scrolls and in Early Christianity.\"\n",
      "\n",
      "   The Thursday afternoon sessions, at 1:30 p.m., include:\n",
      "\n",
      "   -- Devorah Dimant, associate professor of Bible and Ancient Jewish\n",
      "Thought, University of Haifa, on \"Qumran Manuscripts: Library of a\n",
      "Jewish Community;\"\n",
      "   -- Norman Golb, Rosenberger professor of Jewish history and\n",
      "civilization, Oriental Institute, University of Chicago, on \"The\n",
      "Current Status of the Jerusalem Origin of the Scrolls;\"\n",
      "   -- Shmaryahu Talmon, J.L. Magnas professor emeritus of Biblical\n",
      "studies, Hebrew University, Jerusalem, on \"The Essential 'Commune of\n",
      "the Renewed Covenant': How Should Qumran Studies Proceed?\" will close\n",
      "the symposium.\n",
      "\n",
      "   There will be ample time for question and answer periods at the\n",
      "end of each session.\n",
      "\n",
      "   Also on Wednesday, April 21, at 11 a.m.:\n",
      "   The Library of Congress and The Israel Antiquities Authority\n",
      "will hold a lecture by Esther Boyd-Alkalay, consulting conservator,\n",
      "Israel Antiquities Authority, on \"Preserving the Dead Sea Scrolls\"\n",
      "in the Mumford Room, LM-649, James Madison Memorial Building, The\n",
      "Library of Congress, 101 Independence Ave., S.E., Washington, D.C.\n",
      "    ------\n",
      "   NOTE A: For more information about admission to the symposium,\n",
      "please contact, in writing, Dr. Michael Grunberger, head, Hebraic\n",
      "Section, African and Middle Eastern Division, Library of Congress,\n",
      "Washington, D.C. 20540.\n",
      " -30-\n",
      "--\n",
      "Canada Remote Systems - Toronto, Ontario\n",
      "416-629-7000/629-7044\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print twenty_train.target_names\n",
    "\n",
    "print twenty_train.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1079\n"
     ]
    }
   ],
   "source": [
    "print len(twenty_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'and', u'are', u'as', u'be', u'but', u'edu', u'for', u'from', u'god', u'have', u'in', u'is', u'it', u'not', u'of', u'that', u'the', u'this', u'to', u'you']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[26,  1,  0,  8,  0,  0,  3,  9,  0,  1,  8,  2,  0,  0, 44,  1, 57,\n",
       "         0,  7,  0],\n",
       "       [14,  3,  4,  9,  3,  4,  9,  4,  5,  6, 11, 10, 11,  3, 10,  4, 17,\n",
       "         5, 19,  7],\n",
       "       [ 4,  1,  2,  1,  1,  3,  0,  1,  3,  3,  3,  3,  1,  1,  5,  4,  8,\n",
       "         1,  5,  0],\n",
       "       [ 4,  6,  1,  6,  3,  2,  1,  2,  0,  4,  1,  7,  7,  4, 10,  7, 17,\n",
       "         5, 13, 13],\n",
       "       [ 5,  1,  2,  3,  0,  3,  0,  3,  0,  0,  5,  5,  3,  2,  5,  1, 13,\n",
       "         3,  5,  2],\n",
       "       [10, 14,  0,  3,  1,  2,  0,  2,  0,  4, 16, 22,  4,  5, 10, 10, 16,\n",
       "         9, 14, 14],\n",
       "       [17,  7,  5, 13, 15,  5, 10,  2,  5, 12, 16, 33, 26, 17, 17, 26, 39,\n",
       "        10, 42, 14],\n",
       "       [ 2,  2,  3,  2,  1,  4,  2,  1,  1,  1,  1,  1,  9,  1,  7,  3,  4,\n",
       "         3,  3,  3],\n",
       "       [ 1,  2,  0,  0,  1,  2,  0,  2,  0,  0,  5,  1,  0,  1,  2,  5,  8,\n",
       "         0,  5,  4],\n",
       "       [ 3,  0,  0,  0,  0,  1,  0,  1,  0,  0,  1,  0,  2,  0,  1,  1,  1,\n",
       "         0,  2,  1]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(max_features=20)\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape\n",
    "\n",
    "\n",
    "print count_vect.get_feature_names()\n",
    "X_train_counts[:10].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construir dataframe desde vectorizador\n",
    "\n",
    "Queremos armar el dataframe sin ninguna cosa rala ni nada por el estilo\n",
    "\n",
    "Esta vez usamos el vectorizador pero sacando stop words (para que no queden 'and' 'is' y otros yuyos como primeras palabras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   article  atheists  believe  bible  christ  christian  christians  church  \\\n",
      "0        0         0        0      2       0          0           0       0   \n",
      "1        0         0        2      4       0          2           0       1   \n",
      "2        1         0        0      0       0          0           0       0   \n",
      "3        0         1        0      0       0          0           0       0   \n",
      "4        1         1        0      0       0          1           1       0   \n",
      "5        1         0        0      2       0          0           2       3   \n",
      "6        5         0        1      0       0          0           0       0   \n",
      "7        1         0        0      0       0          0           0       0   \n",
      "8        1         0        0      0       0          0           0       0   \n",
      "9        0         0        0      0       0          0           0       0   \n",
      "\n",
      "   com  did   ...    subject  things  think  time  true  truth  university  \\\n",
      "0    1    0   ...          1       0      0     1     0      0          12   \n",
      "1    0    1   ...          1       1      2     6     1      0           0   \n",
      "2    1    0   ...          1       0      0     0     0      0           2   \n",
      "3    1    0   ...          1       1      5     0     0      0           0   \n",
      "4    0    0   ...          1       0      1     0     0      0           0   \n",
      "5    3    0   ...          1       0      2     1     5      6           0   \n",
      "6    0    1   ...          1       1      3     2     6      0           0   \n",
      "7    0    0   ...          1       0      1     0     0      0           2   \n",
      "8    3    0   ...          1       5      2     0     1      0           0   \n",
      "9    0    0   ...          1       0      0     0     0      0           1   \n",
      "\n",
      "   way  world  writes  \n",
      "0    0      0       0  \n",
      "1    1      1       1  \n",
      "2    0      0       2  \n",
      "3    0      0       1  \n",
      "4    0      0       1  \n",
      "5    1      2       2  \n",
      "6    0      3       2  \n",
      "7    0      0       1  \n",
      "8    0      1       2  \n",
      "9    0      0       0  \n",
      "\n",
      "[10 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(max_features=40, stop_words='english')\n",
    "\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape\n",
    "\n",
    "\n",
    "df = pd.DataFrame(X_train_counts.toarray(), columns=count_vect.get_feature_names())\n",
    "\n",
    "print df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD-IDF\n",
    "\n",
    "Occurrence count is a good start but there is an issue: longer documents will have higher average count values than shorter documents, even though they might talk about the same topics.\n",
    "To avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called tf for Term Frequencies.\n",
    "Another refinement on top of tf is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1079, 100)\n",
      "       1993       apr  argument   article  atheism  atheist  atheists  belief  \\\n",
      "0  0.000000  0.000000  0.000000  0.000000      0.0      0.0  0.000000     0.0   \n",
      "1  0.000000  0.000000  0.000000  0.000000      0.0      0.0  0.000000     0.0   \n",
      "2  0.165481  0.000000  0.000000  0.116009      0.0      0.0  0.000000     0.0   \n",
      "3  0.000000  0.000000  0.000000  0.000000      0.0      0.0  0.087735     0.0   \n",
      "4  0.163866  0.169518  0.649793  0.114876      0.0      0.0  0.189245     0.0   \n",
      "5  0.075125  0.077716  0.000000  0.052665      0.0      0.0  0.000000     0.0   \n",
      "6  0.000000  0.000000  0.000000  0.164614      0.0      0.0  0.000000     0.0   \n",
      "7  0.227492  0.235339  0.000000  0.159480      0.0      0.0  0.000000     0.0   \n",
      "8  0.000000  0.000000  0.000000  0.072758      0.0      0.0  0.000000     0.0   \n",
      "9  0.000000  0.000000  0.000000  0.000000      0.0      0.0  0.000000     0.0   \n",
      "\n",
      "    believe     bible  ...    university       use        ve      want  \\\n",
      "0  0.000000  0.185854  ...      0.863465  0.000000  0.000000  0.000000   \n",
      "1  0.131175  0.294119  ...      0.000000  0.084884  0.079334  0.079181   \n",
      "2  0.000000  0.000000  ...      0.270053  0.000000  0.000000  0.000000   \n",
      "3  0.000000  0.000000  ...      0.000000  0.000000  0.000000  0.000000   \n",
      "4  0.000000  0.000000  ...      0.000000  0.000000  0.000000  0.000000   \n",
      "5  0.000000  0.158329  ...      0.000000  0.000000  0.000000  0.000000   \n",
      "6  0.044143  0.000000  ...      0.000000  0.057130  0.000000  0.053292   \n",
      "7  0.000000  0.000000  ...      0.371249  0.000000  0.000000  0.000000   \n",
      "8  0.000000  0.000000  ...      0.000000  0.000000  0.000000  0.000000   \n",
      "9  0.000000  0.000000  ...      0.262388  0.000000  0.000000  0.364906   \n",
      "\n",
      "        way      word     world    writes     wrong  years  \n",
      "0  0.000000  0.000000  0.000000  0.000000  0.000000    0.0  \n",
      "1  0.066951  0.000000  0.073908  0.040770  0.000000    0.0  \n",
      "2  0.000000  0.000000  0.000000  0.193379  0.000000    0.0  \n",
      "3  0.000000  0.000000  0.000000  0.044388  0.091619    0.0  \n",
      "4  0.000000  0.000000  0.000000  0.095745  0.000000    0.0  \n",
      "5  0.072082  0.000000  0.159143  0.087789  0.000000    0.0  \n",
      "6  0.000000  0.058024  0.149229  0.054880  0.056637    0.0  \n",
      "7  0.000000  0.000000  0.000000  0.132922  0.274354    0.0  \n",
      "8  0.000000  0.000000  0.109930  0.121283  0.000000    0.0  \n",
      "9  0.000000  0.000000  0.000000  0.000000  0.000000    0.0  \n",
      "\n",
      "[10 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "transformer = TfidfVectorizer(max_features=100, stop_words='english').fit(twenty_train.data)\n",
    "\n",
    "X_train_tf = transformer.transform(twenty_train.data)\n",
    "print X_train_tf.shape\n",
    "\n",
    "\n",
    "df = pd.DataFrame(X_train_tf.toarray(), columns=transformer.get_feature_names())\n",
    "\n",
    "print df.iloc[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenando clasificador\n",
    "\n",
    "Empezamos Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB().fit(X_train_tf, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf_transformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-1bf5fcb441f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_new_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_new_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_transformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_new_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tf_transformer' is not defined"
     ]
    }
   ],
   "source": [
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tf_transformer.transform(X_new_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_new_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.predict(X_new_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "text_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_clf.predict(docs_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
