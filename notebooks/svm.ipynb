{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando dataframe...\n",
      "Listo\n"
     ]
    }
   ],
   "source": [
    "# Esto agrega al python path el directorio ..\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "import config\n",
    "\n",
    "\n",
    "from dataframe_builder import DataFrameBuilder\n",
    "# Ignorar Warning de sklearn (no es importante)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',\n",
    "                        message='Changing the shape of non-C contiguous array')\n",
    "\n",
    "\n",
    "print \"Creando dataframe...\"\n",
    "\n",
    "\n",
    "builder = DataFrameBuilder()\n",
    "df = builder.build()\n",
    "\n",
    "print \"Listo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "Conjunto a probar\n",
    "\n",
    "1)C in range [0:20] Dice el manual:Por default es uno. If you have a lot of noisy observations you should decrease it. It corresponds to regularize more the estimation.\n",
    "\n",
    "2)Kernels {'rbf ' ,' linear' ,' poly' ,'sigmoid'}  \n",
    "\n",
    "3) para poly podemos hacer un range de degrees, esa opcion va a ser ignorada por los otros kernels asi que podemos hacer degree in range[2:5]. Podemos elegis=r ademas si agregamos un coef0, o sea el termino independiente, no veo por que seria importnte por ahora eso.\n",
    "\n",
    "4)(REVISAR)Por default hay un parametro epsilon=0.1. Epsilon indica un rango de distancia que no sera considerado como penalidad, si el vector esta a espilon--ver\n",
    "\n",
    "decision_function(X)[source]\n",
    "Distance of the samples X to the separating hyperplane.\n",
    "Parameters:\t\n",
    "X : array-like, shape (n_samples, n_features)\n",
    "returns:\t\n",
    "X : array-like, shape (n_samples, n_classes * (n_classes-1) / 2)\n",
    "Returns the decision function of the sample for each class in the model. If decision_function_shape=’ovr’, the shape is (n_samples, n_classes)\n",
    "\n",
    "-Cuando fiteamos este modelo podemos elegir un sample weight, pero no creo que lo necesitemos ya que estan balanceados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics.pairwise import chi2_kernel\n",
    "from sklearn.metrics.pairwise import laplacian_kernel\n",
    "\n",
    "\n",
    "options={\n",
    "    #'kernel': ['rbf ',' linear', 'poly', 'sigmoid', chi2_kernel, laplacian_kernel], # poly tarda mucho, por ahi seria mejor hacerlo aparte\n",
    "    'kernel': 'linear',\n",
    "    'C' : range(1,10),\n",
    "    # 'degree' : range(0,4),\n",
    "    # 'gamma' : np.arange(0,1,0.1)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pendiente: buscar un kernel adecuado, mas alla de las opciones que da sklearn. Recordar para el informe poner la cantidad de vectores de soporte, esos son los vectores sobre los cuales esta basado el hiperplano. Se puede obtener con los atributos.\n",
    "\n",
    "En el manual de sklearn dice: Proper choice of C and gamma is critical to the SVM’s performance. One is advised to use sklearn.grid_search.GridSearchCV with C and gamma spaced exponentially far apart to choose good values.\n",
    "\n",
    "\n",
    "Attributes:\t\n",
    "n_support_ : array-like, dtype=int32, shape = [n_class]\n",
    "Number of support vectors for each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "Scoring precision\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import RandomizedSearchCV, GridSearchCV\n",
    "# Preparo data para clasificar\n",
    "X = df.design_matrix\n",
    "y = df.outcomes\n",
    "\n",
    "scoring_methods = ['precision', 'accuracy', 'f1', 'recall', 'roc_auc']\n",
    "\n",
    "for scoring in scoring_methods:\n",
    "    clf = SVC()\n",
    "\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "    print(\"Scoring {}\".format(scoring))\n",
    "\n",
    "    #search = GridSearchCV(clf, scoring=scoring, param_grid=options, n_jobs=4)\n",
    "    search = clf #RandomizedSearchCV(clf, scoring=\"f1\", param_distributions=options, cv=10, n_jobs=1, n_iter=10)\n",
    "        \n",
    "    search.fit(X, y)\n",
    "    \n",
    "    print \"Mejor combinación: {}\".format(search.best_params_)\n",
    "    print \"Mejor valor: {}\\n\\n\".format(search.best_score_)\n",
    "\n",
    "    \"\"\"\n",
    "    Esto puede no tener mucho sentido. Estamos corriendo nuevamente cross_val_score... \n",
    "    \"\"\"\n",
    "    for other_scorer in scoring_methods:\n",
    "        print \"Valor para {} = {}\".format(\n",
    "            other_scorer,\n",
    "            cross_val_score(search.best_estimator_, X, y, scoring=other_scorer, cv=10).mean()\n",
    "        )\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
