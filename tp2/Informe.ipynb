{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Aprendizaje Automático : Trabajo Práctico 2\n",
    "\n",
    "\n",
    "# Integrantes: Juan Manuel Perez, Mariela Rajngewerc, Tomás Freilij\n",
    "\n",
    "# Resumen:\n",
    "\n",
    "Para el presente trabajo implementamos el algoritmo Q-Learning para la resolucion del juego Cuatro en linea. El objetivo será mostrar cómo un jugador guiado por el algoritmo en cuestión, termina superando ampliamente a uno que elija todos sus movimientos al azar.\n",
    "\n",
    "\n",
    "# Palabras clave: \n",
    "\n",
    "aprendizaje por refuerzos, Q-Learning, cuatro en linea, \n",
    "\n",
    "# Desarrollo\n",
    "\n",
    "El trabajo consistió en armar una implementación del cuatro en línea, que nos permitiera simular fielmente el desarrollo de una partida. Para eso primero lo armamos para que puedan jugar dos humanos.\n",
    "\n",
    "En una segunda etapa, agregamos el jugador random (lo cual no presentó muchas dificultades) y un Qplayer, que va a jugar guiado por el algoritmo en cuestión.\n",
    "\n",
    "Nuestra hipotesis es que cuanto mas entrenamiento realicemos sobre el QPLayer, la tabla Q sera actualizada de forma mas eficiente y asi, dado un estado y una accion el QPlayer sabra cuan conveniente es tomar esa accion dado el estado que se encuentra. \n",
    "\n",
    "Al momento de elegir una accion dado un estado decidimos no usar siempre la experiencia que va adquiriendo el QPlayer sino, con probabilidad epsilon, elegir alguna accion posible al azar. La idea es que el jugador no juegue solo en con las acciones conocidas y que logre experimentar nuevas acciones.\n",
    "\n",
    "Por la escencia del algoritmo por refuerzos elegido: Q-Learning, debimos considerar y experimentar valores para los siguientes hiperparametros: alpha y gamma.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Estrategia \n",
    "\n",
    "Como en el ajedrez, en este juego existe la posibilidad de, antes de hacer un movimiento, explorar las posibles respuestas al mismo, para así tener un mejor análisis de qué jugar, o de en qué situación estamos (si perdiendo, ganando o empatando).\n",
    "\n",
    "Por ejemplo, si estamos en un estado del juego en el que una jugada nos lleva a perder de forma obligada, es evidente que no optaríamos por la misma. A su vez, si una nos lleva a ganar de forma directa, tenemos que ir por ese camino.\n",
    "\n",
    "Es de esta forma, que nuestro Qplayer, haciendo este tipo de análisis, irá aprendiendo. En este caso, aprender es equivalente a aumentar la probabilidad de ganar en una futura partida.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "A. Descripcion y uso de la implementacion\n",
    "\n",
    "B. Calculo de las funciones: Recompensa y Q\n",
    "La funcion recompensa le adjudica a cada (estado, accion) un valor real. Representa el peso que tiene la casilla donde el QPlayer pondra ficha (osea, la accion que ejercera). Decidimos considerar los siguientes resultados posibles:\n",
    "-recompensa_max: en el caso de que el QPlayer resulte ganados\n",
    "-recompensa_media: en el caso que ya hubiera 2 fichas concecutivas del Qplayer, se pueda agregar una tercera y haya un lugar libre donde, en jugadas posteriores podria completarse la cuarta ficha.\n",
    "-recompensa_baja:en el caso que ya hubiera 2 fichas concecutivas del jugador rival, se pueda agregar una tercera y haya un lugar libre donde, en jugadas posteriores podria completarse la cuarta ficha. \n",
    "-recompensa_cero=0\n",
    "\n",
    "\n",
    "-casos que no considere: 1010 por ejemplo, seria bueno poner donde hay cero-\n",
    "C. Calculo \n",
    "\n",
    "Resultados\n",
    "\n",
    "Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Buscar alpha y gamma, hacer el cuadro. Con esto se puede intuir a priori como influye la tasa de aprendizaje.\n",
    "\n",
    "Experimento realizado para epsilon=0,2\n",
    "\n",
    "# 2) Una vez que conseguimos la mejor combinacion, tiro distintos juegos entrenamientos a ver el porcentaje de ganancias\n",
    "25\n",
    "50\n",
    "75\n",
    "100\n",
    "150\n",
    "200\n",
    "250\n",
    "etc\n",
    "\n",
    "Experimento realizado para epsilon=0,2\n",
    "\n",
    "# 3) Pruebo lo mismo que en 2 pero con distintas inicializaciones para Q, podria ser un random entre los distintos rewards y por ahi uno aleatorio de una.\n",
    "\n",
    "\n",
    "# 4) Cambiar la exploracion por la funcion de proba SOFTMAX\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunos resultados:\n",
    "Diccionarios {alpha, gamma: cantidad de ganadas por Qplayer}\n",
    "1)  epsilon 0.2\n",
    "    alphas = [0,0.25,0.5,0.75,1]\n",
    "    gammas = [0,0.25,0.5,0.75,1]\n",
    "    Tirando 100 corridas, obtuvimos\n",
    "    {(0, 0.5): 79, (0.5, 1): 78, (0.75, 0): 72, (0.25, 0.25): 78, (1, 0.5): 85, (0.5, 0.5): 70, (0.75, 0.75): 81, (1, 0.25): 78, (0.75, 0.5): 82, (1, 1): 70, (0.25, 1): 75, (0.25, 0): 76, (0.5, 0): 77, (0, 0.25): 76, (0, 0.75): 82, (0.25, 0.75): 76, (1, 0): 73, (0.5, 0.25): 69, (1, 0.75): 76, (0, 1): 82, (0, 0): 78, (0.75, 1): 78, (0.5, 0.75): 73, (0.75, 0.25): 80, (0.25, 0.5): 82}\n",
    "\n",
    "2)  epsilon 0.2\n",
    "    alphas = [0,1]\n",
    "    gammas = [0,1]\n",
    "   Tirando 500 corridas, obtuvimos:\n",
    "{(0, 1): 387, (1, 0): 391, (0, 0): 364, (1, 1): 393}\n",
    "3)  epsilon 0.2\n",
    "    alphas = \n",
    "    gammas = \n",
    "    Tirando 200 corridas obtuvimos:\n",
    "    {(0.25, 1): 155, (0.25, 0.5): 157, (0.75, 0): 144, (0.75, 0.75): 158, (0.5, 0.75): 150, (0.5, 0.5): 154, (0.5, 0): 158, (0.75, 0.25): 155, (0.25, 0.25): 150, (0.25, 0): 145, (0.75, 0.5): 150, (0.25, 0.75): 150, (1, 0.75): 146, (1, 0.5): 163, (0.5, 0.25): 153, (1, 0): 157, (1, 0.25): 155}\n",
    "    \n",
    "    \n",
    "    FALTAN LOS CASOS (0.5,0) (0.25,0.75) (0.25, 1) (0.75,1) (1,1)\n",
    "    \n",
    "    \n",
    "    \n",
    "   # Empiezo a experimentar con alpha=1 y gamma=0.5 ya que en ambos experimentos fue superadora\n",
    "   QPLAYER VS RANDOM PLAYER\n",
    "   -10 PARTIDAS: {(1, 0.5): 8};{(1, 0.5): 7};{(1, 0.5): 6};{(1, 0.5): 7}\n",
    "   -20 PARTIDAS: {(1, 0.5): 14}\n",
    "   -30 PARTIDAS: {(1, 0.5): 23}\n",
    "   -50 PARTIDAS: {(1, 0.5): 38}\n",
    "   -99 PARTIDAS: {(1, 0.5): 69}\n",
    "   -100 PARTIDAS: {(1, 0.5): 79}; {(1, 0.5): 74};{(1, 0.5): 73};{(1, 0.5): 77};{(1, 0.5): 83}:{(1, 0.5): 72}\n",
    "   -200 PARTIDAS: {(1, 0.5): 149}\n",
    "   -250 PARTIDAS: {(1, 0.5): 197}\n",
    "   -300 PARTIDAS:{(1, 0.5): 232}\n",
    "   -400 PARTIDAS: {(1, 0.5): 303}\n",
    "   -500 partidas: {(1, 0.5): 363};{(1, 0.5): 387}\n",
    "   -1000 partidas: {(1, 0.5): 760}\n",
    "   \n",
    "   "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
