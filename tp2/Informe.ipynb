{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Aprendizaje Automático : Trabajo Práctico 2\n",
    "\n",
    "\n",
    "# Integrantes: Juan Manuel Perez, Mariela Rajngewerc, Tomás Freilij\n",
    "\n",
    "# Resumen:\n",
    "\n",
    "Para el presente trabajo implementamos el algoritmo Q-Learning para la resolucion del juego Cuatro en linea. El objetivo será mostrar cómo un jugador guiado por el algoritmo en cuestión, termina superando ampliamente a uno que elija todos sus movimientos al azar.\n",
    "\n",
    "Como algo adicional, esperamos que si enfrentamos dos jugados guiados por el Q-Learning deberían tender a mantener una misma taza de victorias.\n",
    "\n",
    "\n",
    "# Palabras clave: \n",
    "\n",
    "aprendizaje por refuerzos, Q-Learning, cuatro en linea, \n",
    "\n",
    "# Desarrollo\n",
    "\n",
    "El trabajo consistió en armar una implementación del cuatro en línea, que nos permitiera simular fielmente el desarrollo de una partida. Para eso primero lo armamos para que puedan jugar dos humanos.\n",
    "\n",
    "En una segunda etapa, agregamos el jugador random (lo cual no presentó muchas dificultades) y un Qplayer, que va a jugar guiado por el algoritmo en cuestión.\n",
    "\n",
    "Nuestra hipotesis es que cuanto mas entrenamiento realicemos sobre el QPLayer, la tabla Q sera actualizada de forma mas eficiente y asi, dado un estado y una accion el QPlayer sabra cuan conveniente es tomar esa accion dado el estado que se encuentra. \n",
    "\n",
    "Al momento de elegir una accion dado un estado decidimos no usar siempre la experiencia que va adquiriendo el QPlayer sino, con probabilidad epsilon, elegir alguna accion posible al azar. La idea es que el jugador no juegue solo en con las acciones conocidas y que logre experimentar nuevas acciones.\n",
    "\n",
    "Por la escencia del algoritmo por refuerzos elegido: Q-Learning, debimos considerar y experimentar valores para los siguientes hiperparametros: alpha y gamma.\n",
    "\n",
    "\n",
    "# Estados\n",
    "\n",
    "Los estados están definidos por la configuración de fichas de un tablero. \n",
    "Pensamos definir algún tipo de propiedad más genérica que englobe a muchos momentos de un tablero en un mismo estado. Por ejemplo, en una partida muy avanzada podría no ser necesario analizar las fichas que están en el fondo del tablero, ya que no existe posibilidad de ganar o perder por cómo están dispuestas las mismas.\n",
    "\n",
    "Nos pareció que traería complicaciones implementativas y que comparar dos tableros casilla a casilla no debería ser problemático. Tuvimos en cuenta que esto traería una implementación más fácil, pero mayor consumo de memoria o cómputo a la hora de comparar estados para tomar decisiones.\n",
    "\n",
    "\n",
    "# Tabla Q\n",
    "\n",
    "Por lo dicho anterior, nuestra tabla Q será un mapeo de tuplas (estado, jugada) -> valor.\n",
    "\n",
    "Si tenemos en cuenta que dado un estado, a lo sumo existen siete posibles jugadas, el tamaño de la tabla tendrá un tamaño de O(n), donde n es la cantidad de estados posibles. Como además existen estados inalcanzables, la tabla no será precisamente de esos órdenes de tamaño.\n",
    "\n",
    "Además, la tabla Q no la inicializamos en su totalidad desde el comienzo sino que se hará bajo demanda. En caso de que no se tenga un valor definido para la tabla, se tomará alguno por defecto (quizás al azar) a la hora de los cálculos.\n",
    "De esta forma, la tabla irá creciendo con el tiempo.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Estrategia adoptada \n",
    "\n",
    "Como en el ajedrez, en este juego existe la posibilidad de, antes de hacer un movimiento, explorar las posibles respuestas al mismo. Así tendríamos un mejor análisis de qué jugar, o de en qué situación estamos (si perdiendo, ganando o empatando).\n",
    "\n",
    "Por ejemplo, si estamos en un estado del juego en el que una jugada nos lleva a perder de forma obligada, es evidente que no optaríamos por la misma. A su vez, si una nos lleva a ganar de forma directa, tenemos que ir por ese camino.\n",
    "\n",
    "Es de esta forma, que nuestro Qplayer, haciendo este tipo de análisis, irá aprendiendo. En este caso, aprender es equivalente a aumentar la probabilidad de ganar en una futura partida.\n",
    "\n",
    "La profundidad de análisis (cuántos movimientos a futuro analiza) será algo parametrizable en el Q-Player. En los hechos, resultó que una profundidad de seis o más movimientos a futuro lleva a que cada partida dure alrededor de una hora. Por eso mismo, los análisis serán analizando dos o cuatro.\n",
    "\n",
    "\n",
    "# Ejemplos de análisis\n",
    "\n",
    "Veamos algunos ejemplos de criterios que tomamos para recompensar jugadas nuevas a explorar.\n",
    "\n",
    "Si tenemos dos fichas en línea horizontal y las dos siguientes están libres, parece ser un buen candidato poner una ficha al lado para luego ganar. \n",
    "\n",
    "Se armó una función llamada \"dos_en_linea_y_prox_libre\" que analiza eso\n",
    "\n",
    "# Pseudocódigo de \"encontrar mejor jugada\"\n",
    "\n",
    "La función central está en la elección del mejor movimiento para un estado dado.\n",
    "Para eso, QPlayer tiene implementado \"mejor_accion_para\" que recibe un estado del juego y se encarga de encontrar la mejor respuesta.\n",
    "\n",
    "Veamos el pseudocódigo de la misma:\n",
    "\n",
    "\n",
    "\n",
    "Si número al azar < épsilon\n",
    "\n",
    "    mejor_jugada = elegir jugada al azar\n",
    "    \n",
    "    \n",
    "Si no\n",
    "\n",
    "    Para cada acción posible J del estado actual\n",
    "        Si el valor de Q(estado,J) mejora el de Q(estado,mejor_jugada)\n",
    "            mejor_valor = Q(estado,mejor_jugada)\n",
    "            mejor_jugada = J\n",
    "        \n",
    "       \n",
    "viejo_valor = obtener_valor_en_tabla_q(estado,mejor_jugada)\n",
    "\n",
    "Q(estado,mejor_jugada)] = viejo_valor + alpha * ( recompensa(estado,mejor_jugada) + gamma * mejor_valor - viejo_valor )\n",
    "        \n",
    "\n",
    "# Pseudocódigo de \"encontrar valor q\"\n",
    "\n",
    "Dado un estado S y una posible jugada J:\n",
    "\n",
    "Si jugando J gano\n",
    "    devolver 100\n",
    "Si jugando J gana mi rival\n",
    "    devolver -100 \n",
    "Si jugando J hay empate \n",
    "    devolver 0\n",
    "    \n",
    "Si no\n",
    "    Si llegué a la profunidad de análisis máxima\n",
    "       Para cada acción posibles del rival encontrar su mejor jugada\n",
    "           devolver el valor para esa jugada del rival\n",
    "           \n",
    "    Si queda por recorrer más profundidad\n",
    "    \n",
    "        llamar recursivamente a Q para cada acción mía posibles y quedarme con la mejor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "A. Descripcion y uso de la implementacion\n",
    "\n",
    "B. Calculo de las funciones: Recompensa y Q\n",
    "La funcion recompensa le adjudica a cada (estado, accion) un valor real. Representa el peso que tiene la casilla donde el QPlayer pondra ficha (osea, la accion que ejercera). Decidimos considerar los siguientes resultados posibles:\n",
    "-recompensa_max: en el caso de que el QPlayer resulte ganados\n",
    "-recompensa_media: en el caso que ya hubiera 2 fichas concecutivas del Qplayer, se pueda agregar una tercera y haya un lugar libre donde, en jugadas posteriores podria completarse la cuarta ficha.\n",
    "-recompensa_baja:en el caso que ya hubiera 2 fichas concecutivas del jugador rival, se pueda agregar una tercera y haya un lugar libre donde, en jugadas posteriores podria completarse la cuarta ficha. \n",
    "-recompensa_cero=0\n",
    "\n",
    "\n",
    "-casos que no considere: 1010 por ejemplo, seria bueno poner donde hay cero-\n",
    "C. Calculo \n",
    "\n",
    "Resultados\n",
    "\n",
    "Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Buscar alpha y gamma, hacer el cuadro. Con esto se puede intuir a priori como influye la tasa de aprendizaje.\n",
    "\n",
    "Experimento realizado para epsilon=0,2\n",
    "\n",
    "# 2) Una vez que conseguimos la mejor combinacion, tiro distintos juegos entrenamientos a ver el porcentaje de ganancias\n",
    "25\n",
    "50\n",
    "75\n",
    "100\n",
    "150\n",
    "200\n",
    "250\n",
    "etc\n",
    "\n",
    "Experimento realizado para epsilon=0,2\n",
    "\n",
    "# 3) Pruebo lo mismo que en 2 pero con distintas inicializaciones para Q, podria ser un random entre los distintos rewards y por ahi uno aleatorio de una.\n",
    "\n",
    "\n",
    "# 4) Cambiar la exploracion por la funcion de proba SOFTMAX\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunos resultados:\n",
    "Diccionarios {alpha, gamma: cantidad de ganadas por Qplayer}\n",
    "1)  epsilon 0.2\n",
    "    alphas = [0,0.25,0.5,0.75,1]\n",
    "    gammas = [0,0.25,0.5,0.75,1]\n",
    "    Tirando 100 corridas, obtuvimos\n",
    "    {(0, 0.5): 79, (0.5, 1): 78, (0.75, 0): 72, (0.25, 0.25): 78, (1, 0.5): 85, (0.5, 0.5): 70, (0.75, 0.75): 81, (1, 0.25): 78, (0.75, 0.5): 82, (1, 1): 70, (0.25, 1): 75, (0.25, 0): 76, (0.5, 0): 77, (0, 0.25): 76, (0, 0.75): 82, (0.25, 0.75): 76, (1, 0): 73, (0.5, 0.25): 69, (1, 0.75): 76, (0, 1): 82, (0, 0): 78, (0.75, 1): 78, (0.5, 0.75): 73, (0.75, 0.25): 80, (0.25, 0.5): 82}\n",
    "\n",
    "2)  epsilon 0.2\n",
    "    alphas = [0,1]\n",
    "    gammas = [0,1]\n",
    "   Tirando 500 corridas, obtuvimos:\n",
    "{(0, 1): 387, (1, 0): 391, (0, 0): 364, (1, 1): 393}\n",
    "3)  epsilon 0.2\n",
    "    alphas = \n",
    "    gammas = \n",
    "    Tirando 200 corridas obtuvimos:\n",
    "    {(0.25, 1): 155, (0.25, 0.5): 157, (0.75, 0): 144, (0.75, 0.75): 158, (0.5, 0.75): 150, (0.5, 0.5): 154, (0.5, 0): 158, (0.75, 0.25): 155, (0.25, 0.25): 150, (0.25, 0): 145, (0.75, 0.5): 150, (0.25, 0.75): 150, (1, 0.75): 146, (1, 0.5): 163, (0.5, 0.25): 153, (1, 0): 157, (1, 0.25): 155}\n",
    "    \n",
    "    \n",
    "    FALTAN LOS CASOS (0.5,0) (0.25,0.75) (0.25, 1) (0.75,1) (1,1)\n",
    "    \n",
    "    \n",
    "    \n",
    "   # Empiezo a experimentar con alpha=1 y gamma=0.5 ya que en ambos experimentos fue superadora\n",
    "   QPLAYER VS RANDOM PLAYER\n",
    "   -10 PARTIDAS: {(1, 0.5): 8};{(1, 0.5): 7};{(1, 0.5): 6};{(1, 0.5): 7}\n",
    "   -20 PARTIDAS: {(1, 0.5): 14}\n",
    "   -30 PARTIDAS: {(1, 0.5): 23}\n",
    "   -50 PARTIDAS: {(1, 0.5): 38}\n",
    "   -99 PARTIDAS: {(1, 0.5): 69}\n",
    "   -100 PARTIDAS: {(1, 0.5): 79}; {(1, 0.5): 74};{(1, 0.5): 73};{(1, 0.5): 77};{(1, 0.5): 83}:{(1, 0.5): 72}\n",
    "   -200 PARTIDAS: {(1, 0.5): 149}\n",
    "   -250 PARTIDAS: {(1, 0.5): 197}\n",
    "   -300 PARTIDAS:{(1, 0.5): 232}\n",
    "   -400 PARTIDAS: {(1, 0.5): 303}\n",
    "   -500 partidas: {(1, 0.5): 363};{(1, 0.5): 387}\n",
    "   -1000 partidas: {(1, 0.5): 760}\n",
    "   \n",
    "   "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
