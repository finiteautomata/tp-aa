{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando dataframe...\n",
      "Listo\n"
     ]
    }
   ],
   "source": [
    "# Esto agrega al python path el directorio ..\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "from dataframe_builder import DataFrameBuilder\n",
    "# Ignorar Warning de sklearn (no es importante)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',\n",
    "                        message='Changing the shape of non-C contiguous array')\n",
    "\n",
    "\n",
    "print \"Creando dataframe...\"\n",
    "\n",
    "\n",
    "builder = DataFrameBuilder()\n",
    "df = builder.build()\n",
    "\n",
    "print \"Listo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saco la columna de text porque ocupa mucho espacio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimización de hiperparámetros\n",
    "\n",
    "Busquemos los mejores (posibles) hiperparámetros\n",
    "\n",
    "Para eso, primero veamos qué hiperparámetros nos provee la implementación de SKLearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class RandomForestClassifier in module sklearn.ensemble.forest:\n",
      "\n",
      "class RandomForestClassifier(ForestClassifier)\n",
      " |  A random forest classifier.\n",
      " |  \n",
      " |  A random forest is a meta estimator that fits a number of decision tree\n",
      " |  classifiers on various sub-samples of the dataset and use averaging to\n",
      " |  improve the predictive accuracy and control over-fitting.\n",
      " |  The sub-sample size is always the same as the original\n",
      " |  input sample size but the samples are drawn with replacement if\n",
      " |  `bootstrap=True` (default).\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <forest>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_estimators : integer, optional (default=10)\n",
      " |      The number of trees in the forest.\n",
      " |  \n",
      " |  criterion : string, optional (default=\"gini\")\n",
      " |      The function to measure the quality of a split. Supported criteria are\n",
      " |      \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
      " |      Note: this parameter is tree-specific.\n",
      " |  \n",
      " |  max_features : int, float, string or None, optional (default=\"auto\")\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |      - If int, then consider `max_features` features at each split.\n",
      " |      - If float, then `max_features` is a percentage and\n",
      " |        `int(max_features * n_features)` features are considered at each\n",
      " |        split.\n",
      " |      - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      " |      - If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\").\n",
      " |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |      - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |      Note: this parameter is tree-specific.\n",
      " |  \n",
      " |  max_depth : integer or None, optional (default=None)\n",
      " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |      Ignored if ``max_leaf_nodes`` is not None.\n",
      " |      Note: this parameter is tree-specific.\n",
      " |  \n",
      " |  min_samples_split : integer, optional (default=2)\n",
      " |      The minimum number of samples required to split an internal node.\n",
      " |      Note: this parameter is tree-specific.\n",
      " |  \n",
      " |  min_samples_leaf : integer, optional (default=1)\n",
      " |      The minimum number of samples in newly created leaves.  A split is\n",
      " |      discarded if after the split, one of the leaves would contain less then\n",
      " |      ``min_samples_leaf`` samples.\n",
      " |      Note: this parameter is tree-specific.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, optional (default=0.)\n",
      " |      The minimum weighted fraction of the input samples required to be at a\n",
      " |      leaf node.\n",
      " |      Note: this parameter is tree-specific.\n",
      " |  \n",
      " |  max_leaf_nodes : int or None, optional (default=None)\n",
      " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |      If not None then ``max_depth`` will be ignored.\n",
      " |      Note: this parameter is tree-specific.\n",
      " |  \n",
      " |  bootstrap : boolean, optional (default=True)\n",
      " |      Whether bootstrap samples are used when building trees.\n",
      " |  \n",
      " |  oob_score : bool\n",
      " |      Whether to use out-of-bag samples to estimate\n",
      " |      the generalization error.\n",
      " |  \n",
      " |  n_jobs : integer, optional (default=1)\n",
      " |      The number of jobs to run in parallel for both `fit` and `predict`.\n",
      " |      If -1, then the number of jobs is set to the number of cores.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional (default=None)\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`.\n",
      " |  \n",
      " |  verbose : int, optional (default=0)\n",
      " |      Controls the verbosity of the tree building process.\n",
      " |  \n",
      " |  warm_start : bool, optional (default=False)\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      " |      new forest.\n",
      " |  \n",
      " |  class_weight : dict, list of dicts, \"balanced\", \"balanced_subsample\" or None, optional\n",
      " |  \n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If not given, all classes are supposed to have weight one. For\n",
      " |      multi-output problems, a list of dicts can be provided in the same\n",
      " |      order as the columns of y.\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      " |  \n",
      " |      The \"balanced_subsample\" mode is the same as \"balanced\" except that weights are\n",
      " |      computed based on the bootstrap sample for every tree grown.\n",
      " |  \n",
      " |      For multi-output, the weights of each column of y will be multiplied.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  estimators_ : list of DecisionTreeClassifier\n",
      " |      The collection of fitted sub-estimators.\n",
      " |  \n",
      " |  classes_ : array of shape = [n_classes] or a list of such arrays\n",
      " |      The classes labels (single output problem), or a list of arrays of\n",
      " |      class labels (multi-output problem).\n",
      " |  \n",
      " |  n_classes_ : int or list\n",
      " |      The number of classes (single output problem), or a list containing the\n",
      " |      number of classes for each output (multi-output problem).\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of features when ``fit`` is performed.\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      The number of outputs when ``fit`` is performed.\n",
      " |  \n",
      " |  feature_importances_ : array of shape = [n_features]\n",
      " |      The feature importances (the higher, the more important the feature).\n",
      " |  \n",
      " |  oob_score_ : float\n",
      " |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      " |  \n",
      " |  oob_decision_function_ : array of shape = [n_samples, n_classes]\n",
      " |      Decision function computed with out-of-bag estimate on the training\n",
      " |      set. If n_estimators is small it might be possible that a data point\n",
      " |      was never left out during the bootstrap. In this case,\n",
      " |      `oob_decision_function_` might contain NaN.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  DecisionTreeClassifier, ExtraTreesClassifier\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RandomForestClassifier\n",
      " |      ForestClassifier\n",
      " |      abc.NewBase\n",
      " |      BaseForest\n",
      " |      abc.NewBase\n",
      " |      sklearn.ensemble.base.BaseEnsemble\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.feature_selection.from_model._LearntSelectorMixin\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_estimators=10, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False, class_weight=None)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset([])\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ForestClassifier:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class for X.\n",
      " |      \n",
      " |      The predicted class of an input sample is a vote by the trees in\n",
      " |      the forest, weighted by their probability estimates. That is,\n",
      " |      the predicted class is the one with highest mean probability\n",
      " |      estimate across the trees.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array of shape = [n_samples] or [n_samples, n_outputs]\n",
      " |          The predicted classes.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict class log-probabilities for X.\n",
      " |      \n",
      " |      The predicted class log-probabilities of an input sample is computed as\n",
      " |      the log of the mean predicted class probabilities of the trees in the\n",
      " |      forest.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : array of shape = [n_samples, n_classes], or a list of n_outputs\n",
      " |          such arrays if n_outputs > 1.\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute `classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Predict class probabilities for X.\n",
      " |      \n",
      " |      The predicted class probabilities of an input sample is computed as\n",
      " |      the mean predicted class probabilities of the trees in the forest. The\n",
      " |      class probability of a single tree is the fraction of samples of the same\n",
      " |      class in a leaf.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : array of shape = [n_samples, n_classes], or a list of n_outputs\n",
      " |          such arrays if n_outputs > 1.\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute `classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseForest:\n",
      " |  \n",
      " |  apply(self, X)\n",
      " |      Apply trees in the forest to X, return leaf indices.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array_like, shape = [n_samples, n_estimators]\n",
      " |          For each datapoint x in X and for each tree in the forest,\n",
      " |          return the index of the leaf x ends up in.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Build a forest of trees from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The training input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like, shape = [n_samples] or [n_samples, n_outputs]\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples] or None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. In the case of\n",
      " |          classification, splits are also ignored if they would result in any\n",
      " |          single class carrying a negative weight in either child node.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns self.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseForest:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Return the feature importances (the higher, the more important the\n",
      " |         feature).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : array, shape = [n_features]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble.base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Returns the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Returns iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Returns the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep: boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The former have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.feature_selection.from_model._LearntSelectorMixin:\n",
      " |  \n",
      " |  transform(*args, **kwargs)\n",
      " |      DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      " |      \n",
      " |      Reduce X to its most important features.\n",
      " |      \n",
      " |              Uses ``coef_`` or ``feature_importances_`` to determine the most\n",
      " |              important features.  For models with a ``coef_`` for each class, the\n",
      " |              absolute sum over the classes is used.\n",
      " |      \n",
      " |              Parameters\n",
      " |              ----------\n",
      " |              X : array or scipy sparse matrix of shape [n_samples, n_features]\n",
      " |                  The input samples.\n",
      " |      \n",
      " |              threshold : string, float or None, optional (default=None)\n",
      " |                  The threshold value to use for feature selection. Features whose\n",
      " |                  importance is greater or equal are kept while the others are\n",
      " |                  discarded. If \"median\" (resp. \"mean\"), then the threshold value is\n",
      " |                  the median (resp. the mean) of the feature importances. A scaling\n",
      " |                  factor (e.g., \"1.25*mean\") may also be used. If None and if\n",
      " |                  available, the object attribute ``threshold`` is used. Otherwise,\n",
      " |                  \"mean\" is used by default.\n",
      " |      \n",
      " |              Returns\n",
      " |              -------\n",
      " |              X_r : array of shape [n_samples, n_selected_features]\n",
      " |                  The input samples with only the selected features.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to X and y with optional parameters fit_params\n",
      " |      and returns a transformed version of X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : numpy array of shape [n_samples, n_features]\n",
      " |          Training set.\n",
      " |      \n",
      " |      y : numpy array of shape [n_samples]\n",
      " |          Target values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : numpy array of shape [n_samples, n_features_new]\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "help(RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Opciones:\n",
    "\n",
    "- n_estimators: cantidad de árboles\n",
    "- criterion: gini o entropy. Son dos medidas distintas\n",
    "- splitter: No estoy muy seguro\n",
    "- max_depth: máxima profundidad del árbol\n",
    "- max_features: qué porcentaje de variables tomo a la hora de partir un nodo.\n",
    "  Estas variables se eligen aleatoriamente\n",
    "- min_samples_split: cuántos elementos tengo que tener en un nodo para decidir\n",
    "  partirlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "Scoring precision\n",
      "Mejor combinación: {'n_estimators': 15, 'min_samples_split': 2, 'criterion': 'entropy', 'max_depth': 26, 'max_features': 0.30000000000000004}\n",
      "Mejor valor: 0.968245604638\n",
      "\n",
      "\n",
      "Valor para precision_score = 0.993900632161\n",
      "Valor para accuracy_score = 0.993913204519\n",
      "Valor para f1_score = 0.993912903903\n",
      "Valor para recall_score = 0.993925175948\n",
      "Valor para roc_auc_score = 0.993913205258\n",
      "================================================================================\n",
      "\n",
      "Scoring accuracy\n",
      "Mejor combinación: {'n_estimators': 70, 'min_samples_split': 22, 'criterion': 'entropy', 'max_depth': 26, 'max_features': 0.70000000000000007}\n",
      "Mejor valor: 0.968294339157\n",
      "\n",
      "\n",
      "Valor para precision_score = 0.983425414365\n",
      "Valor para accuracy_score = 0.986171985925\n",
      "Valor para f1_score = 0.986210293031\n",
      "Valor para recall_score = 0.989010989011\n",
      "Valor para roc_auc_score = 0.986172161172\n",
      "================================================================================\n",
      "\n",
      "Scoring f1\n",
      "Mejor combinación: {'n_estimators': 70, 'min_samples_split': 12, 'criterion': 'entropy', 'max_depth': 50, 'max_features': 0.90000000000000013}\n",
      "Mejor valor: 0.969289939464\n",
      "\n",
      "\n",
      "Valor para precision_score = 0.991577391947\n",
      "Valor para accuracy_score = 0.992913142787\n",
      "Valor para f1_score = 0.992922318126\n",
      "Valor para recall_score = 0.994270897642\n",
      "Valor para roc_auc_score = 0.992913226599\n",
      "================================================================================\n",
      "\n",
      "Scoring recall\n",
      "Mejor combinación: {'n_estimators': 55, 'min_samples_split': 2, 'criterion': 'gini', 'max_depth': 34, 'max_features': 0.50000000000000011}\n",
      "Mejor valor: 0.97807122595\n",
      "\n",
      "\n",
      "Valor para precision_score = 0.999802352011\n",
      "Valor para accuracy_score = 0.99956787456\n",
      "Valor para f1_score = 0.999567746477\n",
      "Valor para recall_score = 0.999333251019\n",
      "Valor para roc_auc_score = 0.999567860077\n",
      "================================================================================\n",
      "\n",
      "Scoring roc_auc\n",
      "Mejor combinación: {'n_estimators': 95, 'min_samples_split': 12, 'criterion': 'entropy', 'max_depth': 42, 'max_features': 0.90000000000000013}\n",
      "Mejor valor: 0.9946058784\n",
      "\n",
      "\n",
      "Valor para precision_score = 0.991971036623\n",
      "Valor para accuracy_score = 0.993283536021\n",
      "Valor para f1_score = 0.993292066389\n",
      "Valor para recall_score = 0.994616619336\n",
      "Valor para roc_auc_score = 0.99328361831\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.metrics import precision_score, accuracy_score, f1_score, recall_score, roc_auc_score\n",
    "\n",
    "options = {\n",
    "    'n_estimators': range(10, 100, 5),\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': range(2, 52, 8),\n",
    "    'max_features': np.arange(0.1, 1.0, 0.2),\n",
    "    'min_samples_split': range(2, 102, 10),\n",
    "}\n",
    "\n",
    "# Preparo data para clasificar\n",
    "X = df.design_matrix\n",
    "y = df.outcomes\n",
    "\n",
    "scoring_methods = [\n",
    "    'precision', \n",
    "    'accuracy', \n",
    "    'f1',\n",
    "    'recall', \n",
    "    'roc_auc'\n",
    "]\n",
    "\n",
    "for scoring in scoring_methods:\n",
    "    clf = RandomForestClassifier()\n",
    "\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "    print(\"Scoring {}\".format(scoring))\n",
    "\n",
    "    #search = GridSearchCV(clf, scoring=scoring, param_grid=options, n_jobs=4)\n",
    "    search = RandomizedSearchCV(clf, scoring=scoring, param_distributions=options, n_jobs=4, n_iter=10)\n",
    "    \n",
    "    search.fit(X, y)\n",
    "    \n",
    "    print \"Mejor combinación: {}\".format(search.best_params_)\n",
    "    print \"Mejor valor: {}\\n\\n\".format(search.best_score_)\n",
    "\n",
    "    \"\"\"\n",
    "    Esto puede no tener mucho sentido. Estamos corriendo nuevamente cross_val_score... \n",
    "    \"\"\"\n",
    "    for other_scorer in [precision_score, accuracy_score, f1_score, recall_score, roc_auc_score]:\n",
    "        y_pred = search.best_estimator_.predict(X)\n",
    "        \n",
    "        print \"Valor para {} = {}\".format(\n",
    "            other_scorer.__name__,\n",
    "            other_scorer(y, y_pred)\n",
    "        )\n",
    "        \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
