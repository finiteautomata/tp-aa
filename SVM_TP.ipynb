{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "# import numpy as np\n",
    "# from sklearn.grid_search import GridSearchCV\n",
    "# from sklearn import svm\n",
    "# from dataframe_builder import DataFrameBuilder\n",
    "\n",
    "# ham_txt = json.load(open('data/ham_dev.json'))\n",
    "# spam_txt = json.load(open('data/spam_dev.json'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# builder = DataFrameBuilder()\n",
    "\n",
    "# df = builder.build(ham=ham_txt, spam=spam_txt)\n",
    "\n",
    "# # Preparo data para clasificar\n",
    "# X = df[builder.list_of_attributes].values\n",
    "# y = df['class'] == 'spam'\n",
    "\n",
    "# ham=ham_txt[1:10]\n",
    "\n",
    "\n",
    "\n",
    "# builder_prueba= DataFrameBuilder(spam=spam_txt[1:2000],ham=ham_txt[1:2000])\n",
    "# df_prueba =builder_prueba.build()\n",
    "\n",
    "# X_prueba=df_prueba[builder_prueba.list_of_attributes].values\n",
    "# y_prueba= df_prueba['class']=='spam'\n",
    "\n",
    "# clf_prueba = svm.SVC(gamma=0.001,C=2, kernel='rbf',degree=2,cache_size=6000)\n",
    "# clf_prueba.fit(X_prueba,y_prueba)\n",
    "\n",
    "\n",
    "# #print y_prueba\n",
    "# b=np.random.randint(1,90000)\n",
    "# a=X[b]\n",
    "# print b\n",
    "# print(clf_prueba.predict(a))\n",
    "# print(clf_prueba.predict(X_prueba[2009]))\n",
    "\n",
    "# #Este algoritmo depende fuertemente de los vectores de soporte, solo depende de estos. Es interesante saber cuantos vectores\n",
    "# #soporte hayq de cada clase.\n",
    "# print (clf_prueba.n_support_) # get number of support vectors for each class\n",
    "# #print (clf_prueba.support_) # get indices of support vectors\n",
    "# #print (clf_prueba.support_vectors_) # get support vectors\n",
    "\n",
    "\n",
    "#print(clf.predict(ham_txt [0]))\n",
    "#print(clf.predict(spam_txt [0])) #predict_log_proba--ver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "Conjunto a probar\n",
    "\n",
    "1)C in range [0:20] Dice el manual:Por default es uno. If you have a lot of noisy observations you should decrease it. It corresponds to regularize more the estimation.\n",
    "\n",
    "2)Kernels {'rbf ' ,' linear' ,' poly' ,'sigmoid'}  \n",
    "\n",
    "3) para poly podemos hacer un range de degrees, esa opcion va a ser ignorada por los otros kernels asi que podemos hacer degree in range[2:5]. Podemos elegis=r ademas si agregamos un coef0, o sea el termino independiente, no veo por que seria importnte por ahora eso.\n",
    "\n",
    "4)(REVISAR)Por default hay un parametro epsilon=0.1. Epsilon indica un rango de distancia que no sera considerado como penalidad, si el vector esta a espilon--ver\n",
    "\n",
    "decision_function(X)[source]\n",
    "Distance of the samples X to the separating hyperplane.\n",
    "Parameters:\t\n",
    "X : array-like, shape (n_samples, n_features)\n",
    "returns:\t\n",
    "X : array-like, shape (n_samples, n_classes * (n_classes-1) / 2)\n",
    "Returns the decision function of the sample for each class in the model. If decision_function_shape=’ovr’, the shape is (n_samples, n_classes)\n",
    "\n",
    "-Cuando fiteamos este modelo podemos elegir un sample weight, pero no creo que lo necesitemos ya que estan balanceados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "options={\n",
    "    'kernel' :['rbf ' ,' linear' ,' poly' ,'sigmoid'], # poly tarda mucho, por ahi seria mejor hacerlo aparte\n",
    "    'C' : range(1,10)\n",
    "    'degree' : range(0,4)\n",
    "    'gamma' : range(0,1,0.1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#En sklearn.metrics.pairwise hay otros kernels para probar\n",
    "from sklearn.metrics.pairwise import chi2_kernel\n",
    "from sklearn.metrics.pairwise import laplacian_kernel\n",
    "\n",
    "options={\n",
    "    'kernel' :[chi2_kernel, laplacian_kernel],\n",
    "    'C' : range(1,10)\n",
    "    'gamma' : range(0,1,0.1)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pendiente: buscar un kernel adecuado, mas alla de las opciones que da sklearn. Recordar para el informe poner la cantidad de vectores de soporte, esos son los vectores sobre los cuales esta basado el hiperplano. Se puede obtener con los atributos.\n",
    "\n",
    "En el manual de sklearn dice: Proper choice of C and gamma is critical to the SVM’s performance. One is advised to use sklearn.grid_search.GridSearchCV with C and gamma spaced exponentially far apart to choose good values.\n",
    "\n",
    "\n",
    "Attributes:\t\n",
    "n_support_ : array-like, dtype=int32, shape = [n_class]\n",
    "Number of support vectors for each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
