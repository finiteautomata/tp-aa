{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descripcion:\n",
    "\n",
    "\n",
    "KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=1, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "n_neighbors : int, optional (default = 5)\n",
    "Number of neighbors to use by default for k_neighbors queries.\n",
    "\n",
    "weights : str or callable\n",
    "Por default usa peso uniforme, todos los puntos influencian al nuevo de la misma manera.\n",
    "Si en cambio elegimos 'distance': weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.\n",
    "\n",
    "\n",
    "\n",
    "algorithm : {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, optional\n",
    "Algorithm used to compute the nearest neighbors:\n",
    "‘ball_tree’ will use BallTree\n",
    "‘kd_tree’ will use KDTree\n",
    "‘brute’ will use a brute-force search.\n",
    "‘auto’ will attempt to decide the most appropriate algorithm based on the values passed to fit method.\n",
    "\n",
    "Note: fitting on sparse input will override the setting of this parameter, using brute force.\n",
    "\n",
    "\n",
    "leaf_size : int, optional (default = 30)\n",
    "Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.\n",
    "\n",
    "metric : string or DistanceMetric object (default = ‘minkowski’)\n",
    "the distance metric to use for the tree. The default metric is minkowski, and with p=2 is equivalent to the standard Euclidean metric. See the documentation of the DistanceMetric class for a list of available metrics.\n",
    "\n",
    "p : integer, optional (default = 2)\n",
    "Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
    "\n",
    "metric_params : dict, optional (default = None)\n",
    "Additional keyword arguments for the metric function.\n",
    "\n",
    "n_jobs : int, optional (default = 1)\n",
    "The number of parallel jobs to run for neighbors search. If -1, then the number of jobs is set to the number of CPU cores. Doesn’t affect fit method.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn.neighbors.DistanceMetric\n",
    "\n",
    "\n",
    "Metrics intended for integer-valued vector spaces: Though intended for integer-valued vectors, these are also valid metrics in the case of real-valued vectors.\n",
    "identifier\tclass name\tdistance function\n",
    "“hamming”\tHammingDistance\tN_unequal(x, y) / N_tot\n",
    "No tan beneficioso: que si tenemos un dato donde aparece 10 veces la palabra 'viagra' y otro donde aparece 11 veces, no lo considera como cercano. (Por ahi tengamos que revisar los datos y separar en categorias: 'no aparece', 'aparece hasta 3 veces' y 'aparece mas de 3')\n",
    "“canberra”\tCanberraDistance\tsum(|x - y| / (|x| + |y|))\n",
    "“braycurtis”\tBrayCurtisDistance\tsum(|x - y|) / (sum(|x|) + sum(|y|))\n",
    "\n",
    "\n",
    "En el caso que lleguemos a cambiar a los vectores por vectores booleanos: (no las mire, solo copie porque no creo que hagamos esto.)\n",
    "\n",
    "Metrics intended for boolean-valued vector spaces: Any nonzero entry is evaluated to “True”. In the listings below, the following abbreviations are used:\n",
    "N : number of dimensions\n",
    "NTT : number of dims in which both values are True\n",
    "NTF : number of dims in which the first value is True, second is False\n",
    "NFT : number of dims in which the first value is False, second is True\n",
    "NFF : number of dims in which both values are False\n",
    "NNEQ : number of non-equal dimensions, NNEQ = NTF + NFT\n",
    "NNZ : number of nonzero dimensions, NNZ = NTF + NFT + NTT\n",
    "identifier\tclass name\tdistance function\n",
    "“jaccard”\tJaccardDistance\tNNEQ / NNZ\n",
    "“maching”\tMatchingDistance\tNNEQ / N\n",
    "“dice”\tDiceDistance\tNNEQ / (NTT + NNZ)\n",
    "“kulsinski”\tKulsinskiDistance\t(NNEQ + N - NTT) / (NNEQ + N)\n",
    "“rogerstanimoto”\tRogersTanimotoDistance\t2 * NNEQ / (N + NNEQ)\n",
    "“russellrao”\tRussellRaoDistance\tNNZ / N\n",
    "“sokalmichener”\tSokalMichenerDistance\t2 * NNEQ / (N + NNEQ)\n",
    "“sokalsneath”\tSokalSneathDistance\tNNEQ / (NNEQ + 0.5 * NTT)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Algo que se me ocurrio es que podemos hacer es mezclar un poco, separar el data frame entre los atributos booleanos y los que no lo son. Hacer KNN con ambos datos por separado y despues unir los resultados con algun peso para cada dato, pero no se. Lo vemos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='ball_tree', leaf_size=30, metric='braycurtis',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=2, p=2,\n",
       "           weights='distance')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ejemplo\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "X = [[0], [1], [2], [3],[4],[5]]\n",
    "y = [0, 0, 1, 1,0,0]\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=2,algorithm='ball_tree', weights='distance',metric='braycurtis')\n",
    "neigh.fit(X, y) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "print(neigh.predict([[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "options={\n",
    "    'n_neighbors' : range(10000,30000,1000),\n",
    "    'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "    'metric' : ['canberra', 'braycurtis']\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
