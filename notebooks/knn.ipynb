{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN\n",
    "\n",
    "## Variantes\n",
    "\n",
    "\n",
    "- RadiusNeighborsClassifier: In cases where the data is not uniformly sampled. The user specifies a fixed radius r, such that points in sparser neighborhoods use fewer nearest neighbors for the classification. For high-dimensional parameter spaces, this method becomes less effective due to the so-called “curse of dimensionality”.\n",
    "- KNeighborsClassifier: implements learning based on the k nearest neighbors of each query point, where k is an integer value specified by the user. In general, a larger k suppresses the effects of noise, but makes the classification boundaries less distinct.\n",
    "\n",
    "# sklearn.neighbors.DistanceMetric\n",
    "\n",
    "\n",
    "Metrics intended for integer-valued vector spaces: Though intended for integer-valued vectors, these are also valid metrics in the case of real-valued vectors.\n",
    "identifier\tclass name\tdistance function\n",
    "“hamming”\tHammingDistance\tN_unequal(x, y) / N_tot\n",
    "No tan beneficioso: que si tenemos un dato donde aparece 10 veces la palabra 'viagra' y otro donde aparece 11 veces, no lo considera como cercano. (Por ahi tengamos que revisar los datos y separar en categorias: 'no aparece', 'aparece hasta 3 veces' y 'aparece mas de 3')\n",
    "“canberra”\tCanberraDistance\tsum(|x - y| / (|x| + |y|))\n",
    "“braycurtis”\tBrayCurtisDistance\tsum(|x - y|) / (sum(|x|) + sum(|y|))\n",
    "\n",
    "\n",
    "En el caso que lleguemos a cambiar a los vectores por vectores booleanos: (no las mire, solo copie porque no creo que hagamos esto.)\n",
    "\n",
    "Metrics intended for boolean-valued vector spaces: Any nonzero entry is evaluated to “True”. In the listings below, the following abbreviations are used:\n",
    "N : number of dimensions\n",
    "NTT : number of dims in which both values are True\n",
    "NTF : number of dims in which the first value is True, second is False\n",
    "NFT : number of dims in which the first value is False, second is True\n",
    "NFF : number of dims in which both values are False\n",
    "NNEQ : number of non-equal dimensions, NNEQ = NTF + NFT\n",
    "NNZ : number of nonzero dimensions, NNZ = NTF + NFT + NTT\n",
    "identifier\tclass name\tdistance function\n",
    "“jaccard”\tJaccardDistance\tNNEQ / NNZ\n",
    "“maching”\tMatchingDistance\tNNEQ / N\n",
    "“dice”\tDiceDistance\tNNEQ / (NTT + NNZ)\n",
    "“kulsinski”\tKulsinskiDistance\t(NNEQ + N - NTT) / (NNEQ + N)\n",
    "“rogerstanimoto”\tRogersTanimotoDistance\t2 * NNEQ / (N + NNEQ)\n",
    "“russellrao”\tRussellRaoDistance\tNNZ / N\n",
    "“sokalmichener”\tSokalMichenerDistance\t2 * NNEQ / (N + NNEQ)\n",
    "“sokalsneath”\tSokalSneathDistance\tNNEQ / (NNEQ + 0.5 * NTT)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Algo que se me ocurrio es que podemos hacer es mezclar un poco, separar el data frame entre los atributos booleanos y los que no lo son. Hacer KNN con ambos datos por separado y despues unir los resultados con algun peso para cada dato, pero no se. Lo vemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo archivos...\n",
      "Construyendo datos\n"
     ]
    }
   ],
   "source": [
    "# Esto agrega al python path el directorio ..\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from dataframe_builder import DataFrameBuilder\n",
    "# Ignorar Warning de sklearn (no es importante)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',\n",
    "                        message='Changing the shape of non-C contiguous array')\n",
    "\n",
    "\n",
    "print \"Leyendo archivos...\"\n",
    "ham, spam = json.load(open('../data/ham_dev.json')),json.load(open('../data/spam_dev.json'))\n",
    "\n",
    "print \"Construyendo datos\"\n",
    "builder = DataFrameBuilder()\n",
    "df = builder.build(spam=spam, ham=ham)\n",
    "\n",
    "print \"Listo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimización de hiperparámetros\n",
    "\n",
    "Busquemos los mejores (posibles) hiperparámetros\n",
    "\n",
    "Para eso, primero veamos qué hiperparámetros nos provee la implementación de SKLearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "help(KNeighborsClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# En clase Agustin dijo :\n",
    "\n",
    "- Técnica simple que a veces permite aproximar conceptos muy complejos.\n",
    "- El entrenamiento es muy rápido. (¿Entrenamiento?)\n",
    "- La consulta es muy lenta. (AyED eficientes.)\n",
    "- El modelo (¿modelo?) ocupa mucho espacio en disco.\n",
    "- Para pensar: La distancia se calcula con todos los atributos. ¿Qué pasa si algunos son irrelevantes?\n",
    "\n",
    "Lo unico que se me ocurrio para responder es que se podra ahorrar bastante tiempo si no consideramos los atributos que estan al pepe. Para el informe estaria bueno volver a tirar knn despues de haber hecho pca o ica sobre los atributos y poner un timer a ver que onda.\n",
    "Uds piensan que sera eso?\n",
    "\n",
    "Opciones:\n",
    "\n",
    "- n_neighbors: Cantidad de vecinos (?)\n",
    "- weights: str\n",
    "- splitter: No estoy muy seguro\n",
    "- max_depth: máxima profundidad del árbol\n",
    "- max_features: qué porcentaje de variables tomo a la hora de partir un nodo.\n",
    "  Estas variables se eligen aleatoriamente\n",
    "- min_samples_split: cuántos elementos tengo que tener en un nodo para decidir\n",
    "  partirlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "\n",
    "options = {\n",
    "    'n_neighbors': range(3, 100),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
    "    # p es la p-distancia que usamos\n",
    "    'p': range(1, 10)\n",
    "    \n",
    "}\n",
    "\n",
    "# Preparo data para clasificar\n",
    "X = df[builder.list_of_attributes].values\n",
    "y = df['class'] == 'spam'\n",
    "\n",
    "scoring_methods = ['precision', 'accuracy', 'f1', 'recall', 'roc_auc']\n",
    "\n",
    "for scoring in scoring_methods:\n",
    "    clf = KNeighborsClassifier()\n",
    "\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "    print(\"Scoring {}\".format(scoring))\n",
    "\n",
    "    #search = GridSearchCV(clf, scoring=scoring, param_grid=options, n_jobs=4)\n",
    "    search = RandomizedSearchCV(clf, scoring=scoring, param_distributions=options, n_jobs=4, n_iter=10)\n",
    "    \n",
    "    search.fit(X, y)\n",
    "    \n",
    "    print \"Mejor combinación: {}\".format(search.best_params_)\n",
    "    print \"Mejor valor: {}\\n\\n\".format(search.best_score_)\n",
    "\n",
    "    \"\"\"\n",
    "    Esto puede no tener mucho sentido. Estamos corriendo nuevamente cross_val_score... \n",
    "    \"\"\"\n",
    "    for other_scorer in scoring_methods:\n",
    "        print \"Valor para {} = {}\".format(\n",
    "            other_scorer,\n",
    "            cross_val_score(search.best_estimator_, X, y, scoring=other_scorer, cv=10).mean()\n",
    "        )\n",
    "        \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
